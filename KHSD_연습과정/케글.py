# -*- coding: utf-8 -*-
"""케글.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xCd2HuTKvKcnOvbJCxu-Rcws7qsqZhB5
"""

!pip install konlpy

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
# %matplotlib inline
import matplotlib.pyplot as plt
import seaborn as sns
import re
import urllib.request
from konlpy.tag import Okt
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, LSTM, Embedding
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import load_model
from collections import Counter
from keras import optimizers

train = pd.read_csv(r'/content/케글/train.hate.csv', engine='python', encoding='utf-8', sep=',')
dev = pd.read_csv(r'/content/케글/dev.hate.csv', engine='python', encoding='utf-8', sep=',')
test = pd.read_csv(r'/content/케글/test.hate.no_label.csv', engine='python', encoding='utf-8', sep=',')

train.info()

dev.info()

test.info()

train_dev=pd.concat([train,dev])

train_dev

train_dev['label'].value_counts().plot(kind='bar')

print(train_dev.groupby('label').size().reset_index(name='count'))

train_dev['comments']= train_dev['comments'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
# 한글, 공백 제외한 후, 모두 제거

train_dev[:5]

test = test.reindex(columns = test.columns.tolist() + ["label"])

test['comments']= test['comments'].str.replace("[^ㄱ-ㅎㅏ-ㅣ가-힣 ]","")
# 한글, 공백 제외한 후, 모두 제거

test[:5]

stopwords = ['의','가','이','은','들','는','좀','잘','걍','과','도','를','으로','자','에','와','한','하다']

okt = Okt()
okt.morphs('현재 호텔주인 심정 아 난 마른하늘에 날벼락맞고 호텔망하게생겼는데 누군 계속 추모받네', stem=True)

train_dev['tokenized'] = train_dev['comments'].apply(okt.morphs)
train_dev['tokenized'] = train_dev['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])
test['tokenized'] = test['comments'].apply(okt.morphs)
test['tokenized'] = test['tokenized'].apply(lambda x: [item for item in x if item not in stopwords])

# train_dev 데이터 토큰화
'''
x_train = []
for sentence in train_dev["comments"]:
  temp_x = okt.morphs(sentence, stem=True) #토큰화
  temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거
  x_train.append(temp_x)
'''

print(x_train[:3])

'''
# test 데이터 토큰화
x_test = []
for sentence in test["comments"]:
  temp_x = okt.morphs(sentence, stem=True) #토큰화
  temp_x = [word for word in temp_x if not word in stopwords] # 불용어 제거
  x_test.append(temp_x)
'''

print(x_test[:3])

"""# 단어와 길이 분포 확인

"""

'''
train_dev['label']
train_dev_mapping = pd.Series(['none','offensive','hate'])
label_new = {'none':0, 'offensive':1, 'hate':2}
print(train_dev_mapping.map(label_new))
train_dev_mapping
'''

train_dev=train_dev.replace({"none":0,"offensive":1,"hate":2})
train_dev

'''
train_dev=train_dev.drop(columns=['tokenized'])
'''

none_words = np.hstack(train_dev[train_dev.label == 0]['tokenized'].values)
offensive_words = np.hstack(train_dev[train_dev.label == 1]['tokenized'].values)
hate_words = np.hstack(train_dev[train_dev.label == 2]['tokenized'].values)

# Counter() 사용 단어 빈도수 카운트
none_words_count = Counter(none_words)
print(none_words_count.most_common(200))

offensive_words_count = Counter(offensive_words)
print(offensive_words_count.most_common(200))

hate_words_count = Counter(hate_words)
print(hate_words_count.most_common(200))

fig,(ax1,ax2,ax3) = plt.subplots(nrows=3, ncols=1,figsize=(10,5))
fig.suptitle('comments in texts')
text_len = train_dev[train_dev['label']==0]['tokenized'].map(lambda x:len(x))
ax1.hist(text_len, color='green')
ax1.set_title('none comments')
ax1.set_xlabel('length of samples')
ax1.set_ylabel('number of samples')
print('none의 평균 길이 :', np.mean(text_len))

text_len = train_dev[train_dev['label']==1]['tokenized'].map(lambda x:len(x))
ax2.hist(text_len, color='blue')
ax2.set_title('offensive comments')
ax2.set_xlabel('length of samples')
ax2.set_ylabel('number of samples')
print('offensive의 평균 길이 :', np.mean(text_len))

text_len = train_dev[train_dev['label']==2]['tokenized'].map(lambda x:len(x))
ax3.hist(text_len, color='blue')
ax3.set_title('hate comments')
ax3.set_xlabel('length of samples')
ax3.set_ylabel('number of samples')
print('hate의 평균 길이 :', np.mean(text_len))

X_train = train_dev['tokenized'].values
y_train = train_dev['label'].values
X_test= test['tokenized'].values
y_test = test['label'].values

"""# 정수 인코딩"""

tokenizer = Tokenizer()
# x_train 토큰화 수행
tokenizer.fit_on_texts(X_train)

print(tokenizer.word_index)

# 케라스 토크나이저의 정수 인코딩은 인덱스가 1부터 시작하지만,
# 케라스 원-핫 인코딩에서 배열의 인덱스가 0부터 시작하기 때문에
# 배열의 크기를 실제 단어 집합의 크기보다 +1로 생성해야하므로 미리 +1 선언 

vocab_size = len(tokenizer.word_index)+1
print('단어 집합의 크기 :', vocab_size)

print(tokenizer.word_index)

tokenizer = Tokenizer(vocab_size) 
tokenizer.fit_on_texts(X_train)
# 케라스 토크나이저는 텍스트 시퀸스를 숫자 시퀸스로 변환하여 저장
X_train = tokenizer.texts_to_sequences(X_train)
X_test = tokenizer.texts_to_sequences(X_test)

print(X_train[:3])

print(X_test[:3])

#train_devdptj y_train과 y_test 별도 저장
y_train = np.array(train_dev['label'])
y_test = np.array(test['label'])

"""# 패딩"""

# X_train 패딩
print('커멘트의 최대 길이 :', max(len(l) for l in X_train))
print('커멘트의 평균 길이 :', sum(map(len, X_train))/len(X_train))

plt.hist([len(s) for s in X_train], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

# X_test 패딩
print('커멘트의 최대 길이 :', max(len(l) for l in X_test))
print('커멘트의 평균 길이 :', sum(map(len, X_test))/len(X_test))

plt.hist([len(s) for s in X_test], bins=50)
plt.xlabel('length of samples')
plt.ylabel('number of samples')
plt.show()

# X_train과 X_test의 모든 샘플의 길이를 특정 길이로 맞춰줄 필요가 있음.
# 특정 길이 변수 : max_len으로 정함
# 대부분 커멘트의 내용이 잘리지 않도록 할 수 있는 최적의 max_len의 값 구하기.

def below_threshold_len(max_len, nested_list):
  cnt = 0
  for s in nested_list:
    if(len(s) <= max_len):
        cnt = cnt + 1
  print('전체 샘플 중 길이가 %s 이하인 샘플의 비율: %s'%(max_len, (cnt / len(nested_list))*100))

max_len = 52
below_threshold_len(max_len, X_train)

# 샘플의 길이를 52로 맞춤.
max_len = 52
X_train = pad_sequences(X_train, maxlen=max_len) # 훈련용 커멘트 패딩
X_test = pad_sequences(X_test, maxlen=max_len) # 테스트용 커멘트 패딩

"""# 레이블 값 분포"""

# 레이블 값의 분포
fig, axe = plt.subplots(ncols=1)
fig.set_size_inches(12, 5)
sns.countplot(y_train)

unique_elements, counts_elements = np.unique(y_train, return_counts=True)
print("각 레이블에 대한 빈도수 :")
print(np.asarray((unique_elements, counts_elements)))

y_train = to_categorical(y_train) # 훈련용 커멘트 레이블의 원_핫 인코딩
y_test = to_categorical(y_test) # 테스트용 커멘트의 원_핫 인코딩

# 3개 카테고리 분류 -> 출력층에서 3개의 뉴런사용
# 출력층의 활성화 함수 = 소프트맥스 함수 사용
# 소프트맥스 함수는 각 입력에 대해서 3개의 확률 분포를 만들어냄.
model = Sequential()
model.add(Embedding(vocab_size,100))
model.add(LSTM(128))
model.add(Dense(3, activation='softmax')) 
model.summary()

# 검증 데이터 손실(val_loss)이 증가하면, 과적합 징후므로 
# 검증 데이터 손실이 4회 증가하면 학습을 조기 종료(Early Stopping).
# 또한, ModelCheckpoint를 사용하여 검증 데이터의 정확도(val_acc)가 이전보다
# 좋아질 경우에만 모델을 저장.

es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)
mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)

# 모델 컴파일
# 이 경우 다중 클래스 분류(Multi-Class Classification) 문제이므로 
# 손실 함수로는 categorical_crossentropy를 사용
# categorical_crossentropy는 모델의 예측값과 실제값에 대해서 
# 두 확률 분포 사이의 거리를 최소화하도록 훈련.


model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])

# https://keras.io/ko/callbacks/
# 모델 학습
# validation_data로 X_test와 y_test사용
# val_loss 줄어들다가 증가하는 상황이 오면 과적합으로 판단하기 위함.

history = model.fit(X_train, y_train, batch_size=100, epochs=30, callbacks=[es, mc], validation_data=(X_test, y_test))